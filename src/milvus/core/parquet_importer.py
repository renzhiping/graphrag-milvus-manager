#!/usr/bin/env python3
"""
Parquet -> Milvus æ•°æ®å¯¼å…¥å·¥å…·
è´Ÿè´£ä»ç£ç›˜è¯»å– parquetã€ç”ŸæˆåµŒå…¥å¹¶å†™å…¥é›†åˆ
"""

import logging
import os
from typing import Any, Dict, List

import pandas as pd
from pymilvus import Collection
from dotenv import load_dotenv
from .collection_manager import MilvusCollectionManager
from .constants import DEFAULT_QWEN_EMBEDDING_MODEL
from .embedding_generator import QwenEmbeddingGenerator
from .schema import PARQUET_MAPPING

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)



class MilvusParquetImporter:
    """
    Parquet æ•°æ®å¯¼å…¥å™¨
    è´Ÿè´£ç¦»çº¿å¯¼å…¥ï¼Œä¾èµ–äºå·²å­˜åœ¨çš„é›†åˆ
    """
    
    def __init__(
        self,
        collection_manager: MilvusCollectionManager | None = None,
        batch_size: int = 1000,
        embedding_model: str | None = None,
        embedding_api_key: str | None = None,
        embedding_api_base: str | None = None,
        max_text_length: int = 4000,
    ):
        """
        åˆå§‹åŒ–å¯¼å…¥å™¨
        
        Args:
            collection_manager: Collectionç®¡ç†å™¨å®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°å®ä¾‹
            batch_size: æ‰¹é‡æ’å…¥çš„å¤§å°
            embedding_model: åµŒå…¥æ¨¡å‹åç§°
            embedding_api_key: Qwen/DashScope API Keyï¼Œå¿…é¡»ç”±å¤–éƒ¨æ˜¾å¼ä¼ å…¥
            embedding_api_base: Qwen/DashScope API Baseï¼Œå¯é€‰
        
        æ³¨æ„ï¼š
        - å¦‚æœä¼ å…¥ Noneï¼Œä¼šåˆ›å»ºæ–°çš„ MilvusCollectionManagerï¼Œä½†ä¸ä¼šå»ºç«‹è¿æ¥ï¼Œ
          éœ€è¦å¤–éƒ¨å…ˆå»ºç«‹è¿æ¥ï¼ˆè°ƒç”¨ import_data æ—¶ä¼šè‡ªåŠ¨è¿æ¥ï¼‰ï¼›
        - ä¸ºé¿å…åœ¨ä¸šåŠ¡ä»£ç ä¸­éšå¼è¯»å–ç¯å¢ƒå˜é‡ï¼Œè¿™é‡Œä¸å†ä¸º embedding_api_key æä¾›é»˜è®¤å€¼ã€‚
        """
        self.collection_manager = collection_manager or MilvusCollectionManager()

        # ä½¿ç”¨é»˜è®¤çš„ embedding æ¨¡å‹åç§°ä¸é…ç½®ï¼ˆå¯é€šè¿‡å‚æ•°è¦†ç›–ï¼‰
        self.embedding_model_name = embedding_model or DEFAULT_QWEN_EMBEDDING_MODEL

        if embedding_api_key is None:
            raise ValueError(
                "åˆå§‹åŒ– MilvusParquetImporter éœ€è¦æ˜¾å¼æä¾› embedding_api_keyï¼Œ"
                "è¯·åœ¨åˆ›å»ºæ—¶é€šè¿‡å‚æ•° embedding_api_key ä¼ å…¥ Qwen/DashScope API Keyã€‚"
            )

        self.embedding_generator = QwenEmbeddingGenerator(
            api_key=embedding_api_key,
            api_base=embedding_api_base,
            model=self.embedding_model_name,
            name="milvus_parquet_importer",
        )
        self.max_text_length = max_text_length  # æ–‡æœ¬æœ€å¤§é•¿åº¦é™åˆ¶

        logger.info("åˆå§‹åŒ–Parquetå¯¼å…¥å™¨")
        logger.info(
            "ä½¿ç”¨ Qwen åµŒå…¥æ¨¡å‹: %sï¼Œç»´åº¦=%s",
            self.embedding_model_name,
            self.embedding_generator.dimension,
        )
        logger.info("æ–‡æœ¬æœ€å¤§é•¿åº¦é™åˆ¶: %s å­—ç¬¦", max_text_length)
    
    def _generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """ç”Ÿæˆæ–‡æœ¬çš„embeddingå‘é‡"""
        if not texts:
            return []

        clean_texts = [str(text).strip() if text else "ç©ºå†…å®¹" for text in texts]
        try:
            return self.embedding_generator.embed_batch(clean_texts)
        except Exception as e:  # noqa: BLE001
            logger.error(f"ç”Ÿæˆembeddingå¤±è´¥ï¼Œè¿”å›é›¶å‘é‡: {e}")
            return self.embedding_generator.zero_vectors(len(texts))
    
    def _split_long_text(self, text: str, max_length: int) -> List[str]:
        """æ™ºèƒ½åˆ†å‰²é•¿æ–‡æœ¬ä¸ºå¤šä¸ªå—"""
        if len(text) <= max_length:
            return [text]
        
        chunks = []
        current_chunk = ""
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n\n')
        
        for paragraph in paragraphs:
            # å¦‚æœå½“å‰æ®µè½åŠ ä¸Šç°æœ‰å—ä»åœ¨é™åˆ¶å†…
            if len(current_chunk) + len(paragraph) + 2 <= max_length:
                if current_chunk:
                    current_chunk += '\n\n' + paragraph
                else:
                    current_chunk = paragraph
            else:
                # å¦‚æœå½“å‰å—ä¸ä¸ºç©ºï¼Œä¿å­˜å®ƒ
                if current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = ""
                
                # å¦‚æœæ®µè½æœ¬èº«å°±è¶…è¿‡é™åˆ¶ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
                if len(paragraph) > max_length:
                    # æŒ‰å¥å­åˆ†å‰²
                    sentences = paragraph.replace('ã€‚', 'ã€‚\n').replace('ï¼', 'ï¼\n').replace('ï¼Ÿ', 'ï¼Ÿ\n').split('\n')
                    temp_chunk = ""
                    
                    for sentence in sentences:
                        if len(temp_chunk) + len(sentence) <= max_length:
                            temp_chunk += sentence
                        else:
                            if temp_chunk:
                                chunks.append(temp_chunk.strip())
                            # å¦‚æœå•ä¸ªå¥å­å¤ªé•¿ï¼Œå¼ºåˆ¶åˆ†å‰²
                            if len(sentence) > max_length:
                                for i in range(0, len(sentence), max_length - 100):
                                    chunk_part = sentence[i:i + max_length - 100]
                                    if chunk_part.strip():
                                        chunks.append(chunk_part.strip())
                            else:
                                temp_chunk = sentence
                    
                    if temp_chunk.strip():
                        current_chunk = temp_chunk.strip()
                else:
                    current_chunk = paragraph
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        logger.info(f"é•¿æ–‡æœ¬åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—")
        return chunks
    
    def import_data(
        self,
        parquet_file: str,
        collection_type: str,
        drop_existing: bool = False,
    ) -> bool:
        """
        ä» Parquet æ–‡ä»¶å¯¼å…¥æ•°æ®åˆ° Milvus
        
        Args:
            parquet_file: Parquet æ–‡ä»¶è·¯å¾„
            collection_type: é›†åˆç±»å‹
            drop_existing: æ˜¯å¦åˆ é™¤ç°æœ‰é›†åˆ
        
        Returns:
            bool: å¯¼å…¥æ˜¯å¦æˆåŠŸ
        
        æ³¨æ„ï¼šå‡è®¾ Milvus è¿æ¥å·²ç”±å¤–éƒ¨å»ºç«‹
        """
        try:
            df = pd.read_parquet(parquet_file)
            filename = os.path.basename(parquet_file)
            logger.info(f"è¯»å– {filename}: {df.shape}")
            
            if df.empty:
                logger.warning("æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡")
                return True # è§†ä¸ºæˆåŠŸï¼Œå› ä¸ºæ²¡æœ‰æ•°æ®è¦å¯¼å…¥
            
            # å¦‚æœéœ€è¦ï¼Œåˆ é™¤ç°æœ‰é›†åˆ
            # å½“å‰ MilvusCollectionManager å°šæœªæä¾›å•é›†åˆåˆ é™¤æ¥å£ï¼Œè¿™é‡Œä»…è®°å½•æ—¥å¿—ï¼Œé€»è¾‘ä¿æŒå‘åå…¼å®¹ã€‚
            if drop_existing:
                logger.warning(
                    "å‚æ•° drop_existing=Trueï¼Œä½†å•é›†åˆåˆ é™¤å°šæœªåœ¨ MilvusCollectionManager ä¸­å®ç°ï¼Œè·³è¿‡åˆ é™¤æ­¥éª¤"
                )
            
            # ç¡®ä¿é›†åˆå­˜åœ¨
            if not self.collection_manager.collection_exists(collection_type):
                logger.error(f"é›†åˆ '{collection_type}' ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»ºé›†åˆ")
                return False
            
            # å¯¼å…¥æ•°æ®
            inserted_count = self._import_dataframe_to_collection(df, collection_type)
            return inserted_count > 0
            
        except Exception as e:
            logger.error(f"å¯¼å…¥æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def import_parquet_file(self, file_path: str, collection_type: str) -> int:
        """
        å¯¼å…¥å•ä¸ªParquetæ–‡ä»¶
        
        Args:
            file_path: Parquetæ–‡ä»¶è·¯å¾„
            collection_type: ç›®æ ‡é›†åˆç±»å‹
            
        Returns:
            int: å¯¼å…¥çš„è®°å½•æ•°é‡
        """
        try:
            df = pd.read_parquet(file_path)
            filename = os.path.basename(file_path)
            logger.info(f"è¯»å– {filename}: {df.shape}")
            
            if df.empty:
                logger.warning("æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡")
                return 0
            
            # collection_type å‚æ•°æ˜¯æ˜ å°„ç»“æœï¼Œä½†åŒæ—¶å°†åŸå§‹æ–‡ä»¶åä¼ é€’ç»™ import_dataframeï¼Œ
            # ä»¥ä¾¿ä¿æŒ API ä¸€è‡´æ€§ï¼ˆç›´æ¥è°ƒç”¨ import_dataframe æ—¶ä¹Ÿä»…ä¾èµ–æ–‡ä»¶ååˆ¤æ–­é›†åˆï¼‰
            return self.import_dataframe(df, filename, collection_type=collection_type)
            
        except Exception as e:
            logger.error(f"å¯¼å…¥æ–‡ä»¶å¤±è´¥: {e}")
            return 0

    def import_dataframe(self, df: pd.DataFrame, parquet_filename: str, collection_type: str | None = None) -> int:
        """
        ç›´æ¥å°†DataFrameå†…å®¹å¯¼å…¥Milvusé›†åˆ
        
        Args:
            df: é¢„å¤„ç†åçš„DataFrame
            parquet_filename: å¯¹åº”çš„parquetæ–‡ä»¶åï¼ˆç”¨äºç¡®å®šé›†åˆç±»å‹ï¼‰
            collection_type: å¯é€‰é›†åˆç±»å‹æç¤ºï¼ˆè‹¥å·²çŸ¥å¯ç›´æ¥ä¼ å…¥ï¼‰
            
        Returns:
            int: æˆåŠŸå¯¼å…¥çš„è®°å½•æ•°
        """
        if df is None or df.empty:
            logger.warning("DataFrameä¸ºç©ºï¼Œè·³è¿‡å¯¼å…¥")
            return 0
        
        filename = os.path.basename(parquet_filename)
        resolved_collection_type = collection_type or PARQUET_MAPPING.get(filename)
        if not resolved_collection_type:
            raise ValueError(f"æœªæ‰¾åˆ°æ–‡ä»¶ '{filename}' å¯¹åº”çš„é›†åˆæ˜ å°„ï¼Œè¯·æ£€æŸ¥ PARQUET_MAPPING é…ç½®")
        
        return self._import_dataframe_to_collection(df, resolved_collection_type)

    def _import_dataframe_to_collection(self, df: pd.DataFrame, collection_type: str) -> int:
        """æ‰§è¡Œå®é™…çš„DataFrameå†™å…¥é€»è¾‘"""
        if not self.collection_manager.collection_exists(collection_type):
            raise ValueError(f"é›†åˆ '{collection_type}' ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»ºé›†åˆ")
        
        collection = self.collection_manager.get_collection(collection_type)
        records = self._prepare_records(df, collection_type)
        
        if not records:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯æ’å…¥")
            return 0
        
        inserted_count = self._insert_records(collection, collection_type, records)
        logger.info(f"æ’å…¥ {inserted_count} æ¡æ•°æ®åˆ° {collection_type}")
        return inserted_count
    
    def _prepare_records(self, df: pd.DataFrame, collection_type: str) -> List[Dict[str, Any]]:
        """å‡†å¤‡æ•°æ®è®°å½•å¹¶ç”Ÿæˆembedding"""
        records = []
        texts_to_embed = []
        
        for _, row in df.iterrows():
            record = {"source_id": str(getattr(row, 'id', ''))}
            
            if collection_type == "relationship":
                if hasattr(row, 'description'):
                    desc_content = str(getattr(row, 'description', ''))
                    record.update({"description": desc_content})
                    records.append(record)
                    texts_to_embed.append(desc_content)
            
            elif collection_type == "text_unit":
                if hasattr(row, 'text'):
                    text_content = str(getattr(row, 'text', ''))
                    record.update({"text": text_content})
                    records.append(record)
                    texts_to_embed.append(text_content)
            
            elif collection_type == "entity_title":
                if hasattr(row, 'title'):
                    title_content = str(getattr(row, 'title', ''))
                    record.update({"title": title_content})
                    records.append(record)
                    texts_to_embed.append(title_content)
            
            elif collection_type == "entity_description":
                if hasattr(row, 'title'):
                    title_content = str(getattr(row, 'title', ''))
                    # æ£€æŸ¥æ˜¯å¦æœ‰descriptionå­—æ®µï¼ˆentities.parquetï¼‰æˆ–summaryå­—æ®µï¼ˆcommunity_reports.parquetï¼‰
                    desc_content = ""
                    if hasattr(row, 'description'):
                        desc_content = str(getattr(row, 'description', ''))
                    elif hasattr(row, 'summary'):
                        desc_content = str(getattr(row, 'summary', ''))
                    
                    combined_content = f"{title_content}:{desc_content}"
                    record.update({
                        "title": title_content,
                        "description": desc_content,
                        "title_description": combined_content
                    })
                    records.append(record)
                    texts_to_embed.append(combined_content)
            
            elif collection_type == "community_title":
                if hasattr(row, 'title'):
                    title_content = str(getattr(row, 'title', ''))
                    record.update({"title": title_content})
                    records.append(record)
                    texts_to_embed.append(title_content)
            
            elif collection_type == "community_summary":
                if hasattr(row, 'summary'):
                    summary_content = str(getattr(row, 'summary', ''))
                    record.update({"summary": summary_content})
                    records.append(record)
                    texts_to_embed.append(summary_content)
            
            elif collection_type == "community_full_content":
                if hasattr(row, 'full_content'):
                    full_content = str(getattr(row, 'full_content', ''))
                    record.update({"full_content": full_content})
                    records.append(record)
                    texts_to_embed.append(full_content)
        
        # æ‰¹é‡ç”Ÿæˆembedding
        if records and texts_to_embed:
            logger.info(f"ä¸º{len(texts_to_embed)}æ¡è®°å½•ç”Ÿæˆembeddingå‘é‡...")
            embeddings = self._generate_embeddings(texts_to_embed)
            
            # å°†embeddingæ·»åŠ åˆ°è®°å½•ä¸­
            for i, record in enumerate(records):
                if i < len(embeddings):
                    record["embedding"] = embeddings[i]
                else:
                    record["embedding"] = self.embedding_generator.zero_vector()
        
        return records
    
    def _insert_records(self, collection: Collection, collection_type: str, records: List[Dict[str, Any]]) -> int:
        """æ’å…¥è®°å½•åˆ°é›†åˆ"""
        field_mapping = {
            "relationship": ["source_id", "description", "embedding"],
            "text_unit": ["source_id", "text", "embedding"],
            "entity_title": ["source_id", "title", "embedding"],
            "entity_description": ["source_id", "title", "description", "title_description", "embedding"],
            "community_title": ["source_id", "title", "embedding"],
            "community_summary": ["source_id", "summary", "embedding"],
            "community_full_content": ["source_id", "full_content", "embedding"]
        }
        
        field_names = field_mapping[collection_type]
        insert_data = []
        
        for field in field_names:
            field_values = [item.get(field) for item in records]
            insert_data.append(field_values)
        
        try:
            mr = collection.insert(insert_data)
            collection.flush()
            return len(records)
            
        except Exception as e:
            logger.error(f"æ’å…¥å¤±è´¥: {e}")
            return 0
    
    def import_directory(self, directory_path: str) -> Dict[str, int]:
        """
        å¯¼å…¥æ•´ä¸ªç›®å½•çš„æ‰€æœ‰parquetæ–‡ä»¶
        
        Args:
            directory_path: åŒ…å«parquetæ–‡ä»¶çš„ç›®å½•è·¯å¾„
            
        Returns:
            Dict[str, int]: æ–‡ä»¶ååˆ°å¯¼å…¥è®°å½•æ•°çš„æ˜ å°„
        """
        results = {}
        
        # ç¡®ä¿è¿æ¥åˆ°Milvusï¼ˆè¿æ¥åº”ç”±ä¸Šå±‚ MilvusClient æˆ–è°ƒç”¨æ–¹ç»Ÿä¸€ç®¡ç†ï¼‰
        if not self.collection_manager._connected:
            raise RuntimeError(
                "MilvusCollectionManager å°šæœªè¿æ¥ï¼Œè¯·å…ˆé€šè¿‡ MilvusClient.connect() "
                "æˆ–æ‰‹åŠ¨è°ƒç”¨ collection_manager.connect(host, port) å»ºç«‹è¿æ¥ã€‚"
            )
        
        for filename in os.listdir(directory_path):
            if filename.endswith('.parquet'):
                file_path = os.path.join(directory_path, filename)
                collection_type = PARQUET_MAPPING.get(filename)
                
                if collection_type:
                    count = self.import_parquet_file(file_path, collection_type)
                    results[filename] = count
                else:
                    logger.warning(f"è·³è¿‡æ–‡ä»¶ {filename} (æ— æ˜ å°„é…ç½®)")
        
        return results


def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•å¯¼å…¥åŠŸèƒ½ï¼ˆé€šè¿‡ç»Ÿä¸€çš„ MilvusClientï¼‰"""
    from milvus import MilvusClient

    # è¯»å–ç¯å¢ƒå˜é‡é…ç½®
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(current_dir, "../../../.."))
    load_dotenv(os.path.join(project_root, ".env"))

    # åŸºäºç¯å¢ƒå˜é‡åˆ›å»ºå®¢æˆ·ç«¯
    client = MilvusClient.from_env()
    print(f"â„¹ï¸ ä½¿ç”¨çš„Milvusé…ç½®: {client.host}:{client.port}")

    # å¤ç”¨å®¢æˆ·ç«¯ä¸­çš„ CollectionManager
    collection_manager = client.collection_manager

    try:
        client.connect()

        # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        existing_collections = collection_manager.list_collections()
        if not existing_collections:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•é›†åˆï¼Œè¯·å…ˆè¿è¡Œåˆ›å»ºé›†åˆè„šæœ¬")
            return False

        print(f"âœ… æ‰¾åˆ° {len(existing_collections)} ä¸ªç°æœ‰é›†åˆ")

        # åˆ›å»ºæ•°æ®å¯¼å…¥å™¨
        importer = MilvusParquetImporter(collection_manager)

        # å¯¼å…¥æ•°æ®
        parquet_dir = os.path.join(current_dir, "../tests/parquet")
        if not os.path.exists(parquet_dir):
            print(f"âŒ Parquetç›®å½•ä¸å­˜åœ¨: {parquet_dir}")
            return False

        print(f"ğŸ“‚ ä»ç›®å½•å¯¼å…¥æ•°æ®: {parquet_dir}")
        results = importer.import_directory(parquet_dir)

        # è¾“å‡ºç»“æœ
        print("\nğŸ“Š å¯¼å…¥ç»“æœ:")
        print("=" * 50)
        total = 0
        for file, count in results.items():
            print(f"{file}: {count} æ¡")
            total += count
        print(f"\næ€»è®¡: {total} æ¡æ•°æ®")

        return True

    except Exception as e:
        logger.error(f"å¯¼å…¥è¿‡ç¨‹å‡ºé”™: {e}")
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False

    finally:
        client.disconnect()


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
